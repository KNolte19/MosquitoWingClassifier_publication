{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1734023215935,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "HWCFLn-qIIsJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import skimage as ski\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.models import EfficientNet_B0_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1459,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "oZw7OiBlIdJj",
    "outputId": "5f0e9e70-705d-4864-bef5-bf2cb383c71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "WTTkOBfsIIsM",
    "outputId": "a60b3535-47b6-4cde-c85d-cf6200cd61ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "7rRI7HC-IIsN"
   },
   "outputs": [],
   "source": [
    "# Data Loading Parameters\n",
    "image_height, image_width = 192, 384\n",
    "\n",
    "# Augmentation Parameters\n",
    "brightness = 0.25\n",
    "contrast = 0.25\n",
    "saturation = 0.25\n",
    "hue = 0.25\n",
    "sharpness_factor = 1.25\n",
    "zoom_factor = 1.2\n",
    "degree_factor = 5\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 12\n",
    "batch_size = 16\n",
    "learning_rate = 5e-4\n",
    "warmup_epochs = 3\n",
    "warmup_factor = .1\n",
    "label_smoothing = .1\n",
    "\n",
    "# Define which experiment to run\n",
    "EXPERIMENT = \"bias\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "Ax8Hv6VUr4az"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_list, label_list,\n",
    "                input_transforms,\n",
    "                color_transforms=None,\n",
    "                geo_transforms=None,\n",
    "                geo_trans_vanilla=None,\n",
    "                processing_level=0):\n",
    "\n",
    "        # Initialize the list of files and labels\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        self.input_transforms = input_transforms\n",
    "        self.color_transforms = color_transforms\n",
    "        self.geo_transforms = geo_transforms\n",
    "        self.geo_trans_vanilla = geo_trans_vanilla\n",
    "        self.processing_level = processing_level\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def CLAHE_transform(self, image):\n",
    "            # redice dimension\n",
    "            image = torch.mean(image, dim=0).numpy()\n",
    "            # apply CLAHE\n",
    "            equalized_img = ski.exposure.equalize_adapthist(image, clip_limit=.5, nbins=32) # prevous was clip=.6, nbins=48\n",
    "            # Use mediean filter to reduce noise\n",
    "            equalized_img = ski.filters.median(equalized_img, ski.morphology.disk(2))\n",
    "\n",
    "            return torch.tensor(equalized_img, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.processing_level == 0:\n",
    "            # LVL 0 - No Background Removal, No Augmentation, Color, No CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file / 255\n",
    "            input = input.astype('float32')\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "            if self.geo_trans_vanilla is not None:\n",
    "                input = self.geo_trans_vanilla(input)\n",
    "\n",
    "            return (input, output)\n",
    "\n",
    "        if self.processing_level == 1:\n",
    "            # LVL 1 - No Background Removal, Color Augmentation, Color, No CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file / 255\n",
    "            input = input.astype('float32')\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            if self.color_transforms is not None:\n",
    "                input = self.color_transforms(image=input.astype('float32'))[\"image\"]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "\n",
    "            if self.geo_trans_vanilla is not None:\n",
    "                input = self.geo_trans_vanilla(input)\n",
    "\n",
    "            return (input, output)\n",
    "\n",
    "        if self.processing_level == 2:\n",
    "            # LVL 2 - Background Removal, Color Augmentation, Color ,No CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file[:,:,:3].astype('float32') / 255\n",
    "            mask = file[:,:,3] > 0\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            if self.color_transforms is not None:\n",
    "                input = self.color_transforms(image=input.astype('float32'))[\"image\"]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "            mask = self.input_transforms(mask)\n",
    "\n",
    "            mask = mask.repeat(3, 1, 1)\n",
    "            input[~mask.squeeze(0)] = 0\n",
    "\n",
    "            if self.geo_transforms is not None:\n",
    "                input = self.geo_transforms(input)\n",
    "\n",
    "            return (input, output)\n",
    "\n",
    "        if self.processing_level == 3:\n",
    "            # LVL 3 - Background Removal, Color Augmentation, Greyscale, CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file[:,:,:3].astype('float32') / 255\n",
    "            mask = file[:,:,3] > 0\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            if self.color_transforms is not None:\n",
    "                input = self.color_transforms(image=input.astype('float32'))[\"image\"]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "            mask = self.input_transforms(mask)\n",
    "\n",
    "            input = self.CLAHE_transform(input)\n",
    "\n",
    "            input[~mask.squeeze(0)] = 0\n",
    "            input = input.unsqueeze(0)\n",
    "\n",
    "            if self.geo_transforms is not None:\n",
    "                input = self.geo_transforms(input)\n",
    "\n",
    "            return (input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "6sRX4oUmIIsO"
   },
   "outputs": [],
   "source": [
    "def prepare_data(FOLD, LVL):\n",
    "    # Load the data\n",
    "    # Data is either stored without background removal (LVL: 0, 1) or with background removal (2, 3) as background removal is computationally expensive to do live\n",
    "    path = \"/content/drive/MyDrive/PhD/WingApplication_V3/\"\n",
    "    if LVL < 2:\n",
    "        file_list = np.load(path + \"data/pipeline/{}-images-vanilla.npy\".format(EXPERIMENT))\n",
    "        label_list = np.load(path + \"data/pipeline/{}-labels-vanilla.npy\".format(EXPERIMENT))\n",
    "        fold_list = np.load(path + \"data/pipeline/{}-fold-vanilla.npy\".format(EXPERIMENT))\n",
    "        path_list = np.load(path +\"data/pipeline/{}-path-vanilla.npy\".format(EXPERIMENT))\n",
    "\n",
    "        image_height, image_width = 384, 384\n",
    "\n",
    "    if LVL >= 2:\n",
    "        file_list = np.load(path + \"data/pipeline/{}-images.npy\".format(EXPERIMENT))\n",
    "        label_list = np.load(path + \"data/pipeline/{}-labels.npy\".format(EXPERIMENT))\n",
    "        fold_list = np.load(path + \"data/pipeline/{}-fold.npy\".format(EXPERIMENT))\n",
    "        path_list = np.load(path +\"data/pipeline/{}-path.npy\".format(EXPERIMENT))\n",
    "\n",
    "        image_height, image_width = 192, 384\n",
    "\n",
    "    # One Hot Encode the labels\n",
    "    oh_encoder = OneHotEncoder()\n",
    "    oh_label_list = oh_encoder.fit_transform(label_list.reshape(-1,1)).toarray().astype(np.uint8)\n",
    "\n",
    "    # Split the dataset into train and test based on fold\n",
    "    # -1 is used for OOD data as described in the paper\n",
    "    train_file_list = file_list[(fold_list != FOLD) & (fold_list != -1)]\n",
    "    test_file_list = file_list[(fold_list == FOLD) & (fold_list != -1)]\n",
    "    ood_test_file_list = file_list[(fold_list == -1)]\n",
    "\n",
    "    train_label_list = oh_label_list[(fold_list != FOLD) & (fold_list != -1)]\n",
    "    test_label_list = oh_label_list[(fold_list == FOLD) & (fold_list != -1)]\n",
    "    ood_test_label_list = oh_label_list[(fold_list == -1)]\n",
    "\n",
    "    train_path_list = path_list[(fold_list != FOLD) & (fold_list != -1)]\n",
    "    test_path_list = path_list[(fold_list == FOLD) & (fold_list != -1)]\n",
    "    ood_test_path_list = path_list[(fold_list == -1)]\n",
    "\n",
    "    # Compute Sample Weights\n",
    "    sample_weights = compute_sample_weight('balanced', train_label_list)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(train_file_list), replacement=True)\n",
    "\n",
    "    # Define the transformations\n",
    "    input_trans = transforms.Compose([transforms.ToTensor(), transforms.Resize((image_height, image_width))])\n",
    "\n",
    "    # Updated transforms pipeline\n",
    "    color_trans = A.Compose([\n",
    "        # Image Capture Variance\n",
    "        A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=.5),\n",
    "        A.PlanckianJitter(p=.5),\n",
    "        A.ImageCompression(quality_lower=75, quality_upper=100, p=.25),\n",
    "        A.Defocus(radius=(1, 3), p=.25),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=.25),\n",
    "        A.MotionBlur(blur_limit=(3, 3), p=.25),\n",
    "        A.Downscale(scale_min=0.75, scale_max=1, p=.25),\n",
    "        # Color Changes\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=.5),\n",
    "        A.ChannelDropout(channel_drop_range=(1, 1), p=.25),\n",
    "        # Noise\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), per_channel=True, p=.25),\n",
    "    ])\n",
    "\n",
    "    # Set up two different geometric transformations as the ones with background cannot be cropped to 2:1 ratio\n",
    "    geo_trans_vanilla = transforms.Compose([transforms.v2.RandomZoomOut(fill=0, side_range=(1, 1.1), p=0.75),\n",
    "                                    transforms.Resize((image_width, image_width)), # <- 1:1 ratio\n",
    "                                    transforms.v2.RandomHorizontalFlip(p=0.5),\n",
    "                                    transforms.v2.RandomRotation(degrees=4),])\n",
    "\n",
    "    geo_trans = transforms.Compose([transforms.v2.RandomZoomOut(fill=0, side_range=(1, 1.1), p=0.75),\n",
    "                                    transforms.Resize((image_height, image_width)), # <- 2:1 ratio to reduce unnecessary image size\n",
    "                                    transforms.v2.RandomHorizontalFlip(p=0.5),\n",
    "                                    transforms.v2.RandomRotation(degrees=4),])\n",
    "\n",
    "    # Create an instance of the CustomDataset\n",
    "    train_dataset = CustomDataset(train_file_list, train_label_list, input_trans, color_trans, geo_trans, geo_trans_vanilla, processing_level=LVL)\n",
    "    test_dataset = CustomDataset(test_file_list, test_label_list, input_trans, processing_level=LVL)\n",
    "    ood_dataset = CustomDataset(ood_test_file_list, ood_test_label_list,input_trans, processing_level=LVL)\n",
    "\n",
    "    # Create a DataLoader for the dataset\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=12)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=12)\n",
    "    ood_dataloader = DataLoader(ood_dataset, batch_size=batch_size, num_workers=12)\n",
    "\n",
    "    return train_dataloader, test_dataloader, ood_dataloader, oh_encoder, train_path_list, test_path_list, ood_test_path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217384,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "TDBt5W96IIsP"
   },
   "outputs": [],
   "source": [
    "def load_model(preprocessing_level):\n",
    "\n",
    "    model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "    total_layers = len(list(model.features))  # `features` contains the feature extractor layers\n",
    "    freeze_layers = total_layers // 2         # Calculate the halfway point\n",
    "\n",
    "    # Freeze the first 50% of the layers\n",
    "    for idx, layer in enumerate(model.features):\n",
    "            if idx < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    # Adapt model to preprocesseing LVL\n",
    "    if preprocessing_level == 0:    \n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Output layer\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "    if preprocessing_level == 1:\n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Output layer\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "    if preprocessing_level == 2:\n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Output layer\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "    if preprocessing_level == 3:\n",
    "        \n",
    "        # Modify the first convolution layer to accept a single channel as CLAHE is applied\n",
    "        # Get the current first convolutional layer\n",
    "        original_conv = model.features[0][0]\n",
    "\n",
    "        # Create a new convolutional layer with 1 input channel\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=1,                  # Change to 1 channel\n",
    "            out_channels=original_conv.out_channels,\n",
    "            kernel_size=original_conv.kernel_size,\n",
    "            stride=original_conv.stride,\n",
    "            padding=original_conv.padding,\n",
    "            bias=original_conv.bias is not None\n",
    "        )\n",
    "\n",
    "        # Initialize the new conv layer weights by copying and averaging the weights from the original\n",
    "        with torch.no_grad():\n",
    "            new_conv.weight[:] = original_conv.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Replace the original convolutional layer with the new single-channel conv layer\n",
    "        model.features[0][0] = new_conv\n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Output layer\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217384,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "AcwbqXKSIIsP"
   },
   "outputs": [],
   "source": [
    "# Define scheduler\n",
    "def lr_lambda(current_epoch):\n",
    "    if current_epoch < warmup_epochs:\n",
    "        # Linear warm-up\n",
    "        return warmup_factor + (1 - warmup_factor) * (current_epoch / warmup_epochs)\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        progress = (current_epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, epoch, ce_loss, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = ce_loss(pred, y.argmax(dim=1))\n",
    "        acc = (pred.argmax(dim=1) == y.argmax(dim=1)).float().mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate metrics\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "    # Average metrics for the epoch\n",
    "    avg_loss = total_loss / total_batches\n",
    "    acc = total_acc / total_batches\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss: {avg_loss:.3f}, Accuracy: {acc:.3f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "def test_loop(dataloader, model, ce_loss):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = ce_loss(pred, y.argmax(dim=1))\n",
    "            acc = (pred.argmax(dim=1) == y.argmax(dim=1)).float().mean()\n",
    "\n",
    "            # Calculate metrics\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "    # Average metrics for the epoch\n",
    "    avg_loss = total_loss / total_batches\n",
    "    acc = total_acc / total_batches\n",
    "\n",
    "    print(f\"Test: Loss: {avg_loss:.3f}, Accuracy: {acc:.3f}\")\n",
    "\n",
    "def evaluation(dataloader, model):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "\n",
    "            predictions.append(pred.cpu().detach().numpy())\n",
    "            targets.append(y.cpu().detach().numpy())\n",
    "\n",
    "    return np.concatenate(predictions), np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 675199,
     "status": "ok",
     "timestamp": 1734023892570,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "lUt6z1mZr4a0",
    "outputId": "9bdf948a-43b1-4ef6-b72f-70268ad2092c"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/PhD/WingApplication_V3/\"\n",
    "for preprocessing_level in range(4):\n",
    "    for FOLD in range(5):\n",
    "        print(\"PREPROCESSING LEVEL: \", preprocessing_level, \"FOLD: \", FOLD)\n",
    "        train_dataloader, test_dataloader, ood_dataloader, oh_encoder, train_path_list, test_path_list, ood_test_path_list = prepare_data(FOLD, preprocessing_level)\n",
    "        model = load_model(preprocessing_level)\n",
    "        # Define loss\n",
    "        ce_loss = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        # Define optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "        # TRAIN\n",
    "        for t in range(epochs):\n",
    "            train_loop(train_dataloader, model, t, ce_loss, optimizer, scheduler)\n",
    "            test_loop(test_dataloader, model, ce_loss)\n",
    "            test_loop(ood_dataloader, model, ce_loss)\n",
    "            print(\"----------------------------\")\n",
    "\n",
    "        # TEST & SAVE\n",
    "        test_df = pd.DataFrame()\n",
    "\n",
    "        predictions, targets = evaluation(test_dataloader, model)\n",
    "        predictions_ood, targets_ood = evaluation(ood_dataloader, model)\n",
    "\n",
    "        test_df[\"PATH\"] = list(test_path_list) + list(ood_test_path_list)\n",
    "        test_df[\"PRED\"] = list(oh_encoder.inverse_transform(predictions)) + list(oh_encoder.inverse_transform(predictions_ood))\n",
    "        test_df[\"TARGET\"] = list(oh_encoder.inverse_transform(targets)) + list(oh_encoder.inverse_transform(targets_ood))\n",
    "\n",
    "        path = \"/content/drive/MyDrive/PhD/WingApplication_V3/\"\n",
    "        pd.to_pickle(test_df, path + \"results/{}/test_df_{}_{}-{}.pkl\".format(EXPERIMENT, EXPERIMENT, FOLD, preprocessing_level))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_v2",
   "language": "python",
   "name": "pytorch_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
