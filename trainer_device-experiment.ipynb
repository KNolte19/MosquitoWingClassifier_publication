{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1734023215935,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "HWCFLn-qIIsJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import skimage as ski\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision.models import EfficientNet_B0_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1459,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "oZw7OiBlIdJj",
    "outputId": "5f0e9e70-705d-4864-bef5-bf2cb383c71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "WTTkOBfsIIsM",
    "outputId": "a60b3535-47b6-4cde-c85d-cf6200cd61ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "7rRI7HC-IIsN"
   },
   "outputs": [],
   "source": [
    "# Data Loading Parameters\n",
    "image_height, image_width = 192, 384\n",
    "\n",
    "# Augmentation Parameters\n",
    "brightness = 0.25\n",
    "contrast = 0.25\n",
    "saturation = 0.25\n",
    "hue = 0.25\n",
    "sharpness_factor = 1.25\n",
    "zoom_factor = 1.2\n",
    "degree_factor = 5\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 12\n",
    "batch_size = 16\n",
    "learning_rate = 5e-4\n",
    "warmup_epochs = 3\n",
    "warmup_factor = .1\n",
    "label_smoothing = .1\n",
    "\n",
    "EXPERIMENT = \"bias\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "Ax8Hv6VUr4az"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_list, label_list,\n",
    "                input_transforms,\n",
    "                color_transforms=None,\n",
    "                geo_transforms=None,\n",
    "                geo_trans_vanilla=None,\n",
    "                processing_level=0):\n",
    "\n",
    "        # Initialize the list of files and labels\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        self.input_transforms = input_transforms\n",
    "        self.color_transforms = color_transforms\n",
    "        self.geo_transforms = geo_transforms\n",
    "        self.geo_trans_vanilla = geo_trans_vanilla\n",
    "        self.processing_level = processing_level\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def CLAHE_transform(self, image):\n",
    "            # redice dimension\n",
    "            image = torch.mean(image, dim=0).numpy()\n",
    "            # apply CLAHE\n",
    "            equalized_img = ski.exposure.equalize_adapthist(image, clip_limit=.5, nbins=32) # prevous was clip=.6, nbins=48\n",
    "            # Use mediean filter to reduce noise\n",
    "            equalized_img = ski.filters.median(equalized_img, ski.morphology.disk(2))\n",
    "\n",
    "            return torch.tensor(equalized_img, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.processing_level == 0:\n",
    "            # LVL 0 - No Background Removal, No Augmentation, Color, No CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file / 255\n",
    "            input = input.astype('float32')\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "            if self.geo_trans_vanilla is not None:\n",
    "                input = self.geo_trans_vanilla(input)\n",
    "\n",
    "            return (input, output)\n",
    "\n",
    "        if self.processing_level == 1:\n",
    "            # LVL 1 - No Background Removal, Color Augmentation, Color, No CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file / 255\n",
    "            input = input.astype('float32')\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            if self.color_transforms is not None:\n",
    "                input = self.color_transforms(image=input.astype('float32'))[\"image\"]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "\n",
    "            if self.geo_trans_vanilla is not None:\n",
    "                input = self.geo_trans_vanilla(input)\n",
    "\n",
    "            return (input, output)\n",
    "\n",
    "        if self.processing_level == 2:\n",
    "            # LVL 2 - Background Removal, Color Augmentation, Color ,No CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file[:,:,:3].astype('float32') / 255\n",
    "            mask = file[:,:,3] > 0\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            if self.color_transforms is not None:\n",
    "                input = self.color_transforms(image=input.astype('float32'))[\"image\"]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "            mask = self.input_transforms(mask)\n",
    "\n",
    "            mask = mask.repeat(3, 1, 1)\n",
    "            input[~mask.squeeze(0)] = 0\n",
    "\n",
    "            if self.geo_transforms is not None:\n",
    "                input = self.geo_transforms(input)\n",
    "\n",
    "            return (input, output)\n",
    "\n",
    "        if self.processing_level == 3:\n",
    "            # LVL 3 - Background Removal, Color Augmentation, Greyscale, CLAHE\n",
    "            file = self.file_list[idx]\n",
    "            input = file[:,:,:3].astype('float32') / 255\n",
    "            mask = file[:,:,3] > 0\n",
    "            output = self.label_list[idx]\n",
    "\n",
    "            if self.color_transforms is not None:\n",
    "                input = self.color_transforms(image=input.astype('float32'))[\"image\"]\n",
    "\n",
    "            input = self.input_transforms(input)\n",
    "            mask = self.input_transforms(mask)\n",
    "\n",
    "            input = self.CLAHE_transform(input)\n",
    "\n",
    "            input[~mask.squeeze(0)] = 0\n",
    "            input = input.unsqueeze(0)\n",
    "\n",
    "            if self.geo_transforms is not None:\n",
    "                input = self.geo_transforms(input)\n",
    "\n",
    "            return (input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1734023217383,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "6sRX4oUmIIsO"
   },
   "outputs": [],
   "source": [
    "def prepare_data(FOLD, LVL):\n",
    "    path = \"/content/drive/MyDrive/PhD/WingApplication_V3/\"\n",
    "    if LVL < 2:\n",
    "        file_list = np.load(path + \"data/pipeline/{}-images-vanilla.npy\".format(EXPERIMENT))\n",
    "        label_list = np.load(path + \"data/pipeline/{}-labels-vanilla.npy\".format(EXPERIMENT))\n",
    "        fold_list = np.load(path + \"data/pipeline/{}-fold-vanilla.npy\".format(EXPERIMENT))\n",
    "        path_list = np.load(path +\"data/pipeline/{}-path-vanilla.npy\".format(EXPERIMENT))\n",
    "\n",
    "        image_height, image_width = 384, 384\n",
    "\n",
    "    if LVL >= 2:\n",
    "        file_list = np.load(path + \"data/pipeline/{}-images.npy\".format(EXPERIMENT))\n",
    "        label_list = np.load(path + \"data/pipeline/{}-labels.npy\".format(EXPERIMENT))\n",
    "        fold_list = np.load(path + \"data/pipeline/{}-fold.npy\".format(EXPERIMENT))\n",
    "        path_list = np.load(path +\"data/pipeline/{}-path.npy\".format(EXPERIMENT))\n",
    "\n",
    "        image_height, image_width = 192, 384\n",
    "\n",
    "    oh_encoder = OneHotEncoder()\n",
    "    oh_label_list = oh_encoder.fit_transform(label_list.reshape(-1,1)).toarray().astype(np.uint8)\n",
    "\n",
    "    # Split the dataset into train and test based on fold\n",
    "    train_file_list = file_list[(fold_list != FOLD) & (fold_list != -1)]\n",
    "    test_file_list = file_list[(fold_list == FOLD) & (fold_list != -1)]\n",
    "    ood_test_file_list = file_list[(fold_list == -1)]\n",
    "\n",
    "    train_label_list = oh_label_list[(fold_list != FOLD) & (fold_list != -1)]\n",
    "    test_label_list = oh_label_list[(fold_list == FOLD) & (fold_list != -1)]\n",
    "    ood_test_label_list = oh_label_list[(fold_list == -1)]\n",
    "\n",
    "    train_path_list = path_list[(fold_list != FOLD) & (fold_list != -1)]\n",
    "    test_path_list = path_list[(fold_list == FOLD) & (fold_list != -1)]\n",
    "    ood_test_path_list = path_list[(fold_list == -1)]\n",
    "\n",
    "    # Compute Sample Weights\n",
    "    sample_weights = compute_sample_weight('balanced', train_label_list)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(train_file_list), replacement=True)\n",
    "\n",
    "    # Define the transformations\n",
    "    input_trans = transforms.Compose([transforms.ToTensor(), transforms.Resize((image_height, image_width))])\n",
    "\n",
    "    # Updated transforms pipeline\n",
    "    color_trans = A.Compose([\n",
    "        # Image Capture Variance\n",
    "        A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=.5),\n",
    "        A.PlanckianJitter(p=.5),\n",
    "        A.ImageCompression(quality_lower=75, quality_upper=100, p=.25),\n",
    "        A.Defocus(radius=(1, 3), p=.25),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=.25),\n",
    "        A.MotionBlur(blur_limit=(3, 3), p=.25),\n",
    "        A.Downscale(scale_min=0.75, scale_max=1, p=.25),\n",
    "        # Color Changes\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=.5),\n",
    "        A.ChannelDropout(channel_drop_range=(1, 1), p=.25),\n",
    "        # Noise\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), per_channel=True, p=.25),\n",
    "    ])\n",
    "\n",
    "    geo_trans_vanilla = transforms.Compose([transforms.v2.RandomZoomOut(fill=0, side_range=(1, 1.1), p=0.75),\n",
    "                                    transforms.Resize((image_width, image_width)),\n",
    "                                    transforms.v2.RandomHorizontalFlip(p=0.5),\n",
    "                                    transforms.v2.RandomRotation(degrees=4),])\n",
    "\n",
    "    geo_trans = transforms.Compose([transforms.v2.RandomZoomOut(fill=0, side_range=(1, 1.1), p=0.75),\n",
    "                                    transforms.Resize((image_height, image_width)),\n",
    "                                    transforms.v2.RandomHorizontalFlip(p=0.5),\n",
    "                                    transforms.v2.RandomRotation(degrees=4),])\n",
    "\n",
    "    # Create an instance of the CustomDataset\n",
    "    train_dataset = CustomDataset(train_file_list, train_label_list, input_trans, color_trans, geo_trans, geo_trans_vanilla, processing_level=LVL)\n",
    "    test_dataset = CustomDataset(test_file_list, test_label_list, input_trans, processing_level=LVL)\n",
    "    ood_dataset = CustomDataset(ood_test_file_list, ood_test_label_list,input_trans, processing_level=LVL)\n",
    "\n",
    "    # Create a DataLoader for the dataset\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=12)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=12)\n",
    "    ood_dataloader = DataLoader(ood_dataset, batch_size=batch_size, num_workers=12)\n",
    "\n",
    "    return train_dataloader, test_dataloader, ood_dataloader, oh_encoder, train_path_list, test_path_list, ood_test_path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217384,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "TDBt5W96IIsP"
   },
   "outputs": [],
   "source": [
    "def load_model(preprocessing_level):\n",
    "    # Adapt model to preprocesseing LVL\n",
    "    if preprocessing_level == 0:\n",
    "        model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "         # Calculate the split point for the feature extractor layers\n",
    "        total_layers = len(list(model.features))  # `features` contains the feature extractor layers\n",
    "        freeze_layers = total_layers // 2         # Calculate the halfway point\n",
    "\n",
    "        # Freeze the first 50% of the layers\n",
    "        for idx, layer in enumerate(model.features):\n",
    "            if idx < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Single output for binary classification\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "    if preprocessing_level == 1:\n",
    "        model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "         # Calculate the split point for the feature extractor layers\n",
    "        total_layers = len(list(model.features))  # `features` contains the feature extractor layers\n",
    "        freeze_layers = total_layers // 2         # Calculate the halfway point\n",
    "\n",
    "        # Freeze the first 50% of the layers\n",
    "        for idx, layer in enumerate(model.features):\n",
    "            if idx < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Single output for binary classification\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "    if preprocessing_level == 2:\n",
    "        model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "        # Calculate the split point for the feature extractor layers\n",
    "        total_layers = len(list(model.features))  # `features` contains the feature extractor layers\n",
    "        freeze_layers = total_layers // 2         # Calculate the halfway point\n",
    "\n",
    "        # Freeze the first 50% of the layers\n",
    "        for idx, layer in enumerate(model.features):\n",
    "            if idx < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Single output for binary classification\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "    if preprocessing_level == 3:\n",
    "        model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "         # Calculate the split point for the feature extractor layers\n",
    "        total_layers = len(list(model.features))  # `features` contains the feature extractor layers\n",
    "        freeze_layers = total_layers // 2         # Calculate the halfway point\n",
    "\n",
    "        # Freeze the first 50% of the layers\n",
    "        for idx, layer in enumerate(model.features):\n",
    "            if idx < freeze_layers:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Modify the first convolution layer to accept a single channel\n",
    "        # Get the current first convolutional layer\n",
    "        original_conv = model.features[0][0]\n",
    "\n",
    "        # Create a new convolutional layer with 1 input channel\n",
    "        new_conv = nn.Conv2d(\n",
    "            in_channels=1,                  # Change to 1 channel\n",
    "            out_channels=original_conv.out_channels,\n",
    "            kernel_size=original_conv.kernel_size,\n",
    "            stride=original_conv.stride,\n",
    "            padding=original_conv.padding,\n",
    "            bias=original_conv.bias is not None\n",
    "        )\n",
    "\n",
    "        # Initialize the new conv layer weights by copying and averaging the weights from the original\n",
    "        with torch.no_grad():\n",
    "            new_conv.weight[:] = original_conv.weight.mean(dim=1, keepdim=True)\n",
    "\n",
    "        # Replace the original convolutional layer with the new single-channel conv layer\n",
    "        model.features[0][0] = new_conv\n",
    "\n",
    "        # Modify the classifier to replace the classification head with a dropout and a single dense layer\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),            # Dropout layer\n",
    "            nn.Linear(num_features, 4)    # Single output for binary classification\n",
    "        )\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1734023217384,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "AcwbqXKSIIsP"
   },
   "outputs": [],
   "source": [
    "# Define scheduler\n",
    "def lr_lambda(current_epoch):\n",
    "    if current_epoch < warmup_epochs:\n",
    "        # Linear warm-up\n",
    "        return warmup_factor + (1 - warmup_factor) * (current_epoch / warmup_epochs)\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        progress = (current_epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, epoch, ce_loss, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = ce_loss(pred, y.argmax(dim=1))\n",
    "        acc = (pred.argmax(dim=1) == y.argmax(dim=1)).float().mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate metrics\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "    # Average metrics for the epoch\n",
    "    avg_loss = total_loss / total_batches\n",
    "    acc = total_acc / total_batches\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss: {avg_loss:.3f}, Accuracy: {acc:.3f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "def test_loop(dataloader, model, ce_loss):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = ce_loss(pred, y.argmax(dim=1))\n",
    "            acc = (pred.argmax(dim=1) == y.argmax(dim=1)).float().mean()\n",
    "\n",
    "            # Calculate metrics\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "    # Average metrics for the epoch\n",
    "    avg_loss = total_loss / total_batches\n",
    "    acc = total_acc / total_batches\n",
    "\n",
    "    print(f\"Test: Loss: {avg_loss:.3f}, Accuracy: {acc:.3f}\")\n",
    "\n",
    "def evaluation(dataloader, model):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "\n",
    "            predictions.append(pred.cpu().detach().numpy())\n",
    "            targets.append(y.cpu().detach().numpy())\n",
    "\n",
    "    return np.concatenate(predictions), np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 675199,
     "status": "ok",
     "timestamp": 1734023892570,
     "user": {
      "displayName": "Kristopher Nolte",
      "userId": "15304813975192665624"
     },
     "user_tz": -60
    },
    "id": "lUt6z1mZr4a0",
    "outputId": "9bdf948a-43b1-4ef6-b72f-70268ad2092c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING LEVEL:  2 FOLD:  0\n",
      "Epoch 0: Loss: 1.227, Accuracy: 0.517\n",
      "Test: Loss: 0.796, Accuracy: 0.942\n",
      "Test: Loss: 1.694, Accuracy: 0.103\n",
      "----------------------------\n",
      "Epoch 1: Loss: 0.669, Accuracy: 0.858\n",
      "Test: Loss: 0.410, Accuracy: 0.983\n",
      "Test: Loss: 2.700, Accuracy: 0.013\n",
      "----------------------------\n",
      "Epoch 2: Loss: 0.548, Accuracy: 0.907\n",
      "Test: Loss: 0.391, Accuracy: 0.997\n",
      "Test: Loss: 2.312, Accuracy: 0.083\n",
      "----------------------------\n",
      "Epoch 3: Loss: 0.524, Accuracy: 0.928\n",
      "Test: Loss: 0.429, Accuracy: 0.978\n",
      "Test: Loss: 2.969, Accuracy: 0.021\n",
      "----------------------------\n",
      "Epoch 4: Loss: 0.499, Accuracy: 0.934\n",
      "Test: Loss: 0.381, Accuracy: 0.994\n",
      "Test: Loss: 2.639, Accuracy: 0.030\n",
      "----------------------------\n",
      "Epoch 5: Loss: 0.499, Accuracy: 0.937\n",
      "Test: Loss: 0.498, Accuracy: 0.933\n",
      "Test: Loss: 3.034, Accuracy: 0.018\n",
      "----------------------------\n",
      "Epoch 6: Loss: 0.436, Accuracy: 0.966\n",
      "Test: Loss: 0.369, Accuracy: 1.000\n",
      "Test: Loss: 3.056, Accuracy: 0.098\n",
      "----------------------------\n",
      "Epoch 7: Loss: 0.424, Accuracy: 0.970\n",
      "Test: Loss: 0.366, Accuracy: 1.000\n",
      "Test: Loss: 3.069, Accuracy: 0.022\n",
      "----------------------------\n",
      "Epoch 8: Loss: 0.408, Accuracy: 0.981\n",
      "Test: Loss: 0.361, Accuracy: 1.000\n",
      "Test: Loss: 2.916, Accuracy: 0.022\n",
      "----------------------------\n",
      "Epoch 9: Loss: 0.405, Accuracy: 0.982\n",
      "Test: Loss: 0.361, Accuracy: 1.000\n",
      "Test: Loss: 2.922, Accuracy: 0.027\n",
      "----------------------------\n",
      "Epoch 10: Loss: 0.393, Accuracy: 0.988\n",
      "Test: Loss: 0.358, Accuracy: 1.000\n",
      "Test: Loss: 2.875, Accuracy: 0.023\n",
      "----------------------------\n",
      "Epoch 11: Loss: 0.393, Accuracy: 0.981\n",
      "Test: Loss: 0.358, Accuracy: 1.000\n",
      "Test: Loss: 2.902, Accuracy: 0.022\n",
      "----------------------------\n",
      "PREPROCESSING LEVEL:  2 FOLD:  1\n",
      "Epoch 0: Loss: 1.214, Accuracy: 0.525\n",
      "Test: Loss: 0.795, Accuracy: 0.890\n",
      "Test: Loss: 1.652, Accuracy: 0.030\n",
      "----------------------------\n",
      "Epoch 1: Loss: 0.679, Accuracy: 0.859\n",
      "Test: Loss: 0.475, Accuracy: 0.944\n",
      "Test: Loss: 2.276, Accuracy: 0.060\n",
      "----------------------------\n",
      "Epoch 2: Loss: 0.555, Accuracy: 0.904\n",
      "Test: Loss: 0.419, Accuracy: 0.963\n",
      "Test: Loss: 2.885, Accuracy: 0.014\n",
      "----------------------------\n",
      "Epoch 3: Loss: 0.526, Accuracy: 0.916\n",
      "Test: Loss: 0.419, Accuracy: 0.975\n",
      "Test: Loss: 2.580, Accuracy: 0.050\n",
      "----------------------------\n",
      "Epoch 4: Loss: 0.473, Accuracy: 0.940\n",
      "Test: Loss: 0.375, Accuracy: 0.994\n",
      "Test: Loss: 2.692, Accuracy: 0.111\n",
      "----------------------------\n",
      "Epoch 5: Loss: 0.462, Accuracy: 0.959\n",
      "Test: Loss: 0.395, Accuracy: 0.981\n",
      "Test: Loss: 2.720, Accuracy: 0.093\n",
      "----------------------------\n",
      "Epoch 6: Loss: 0.420, Accuracy: 0.974\n",
      "Test: Loss: 0.367, Accuracy: 0.994\n",
      "Test: Loss: 2.769, Accuracy: 0.024\n",
      "----------------------------\n",
      "Epoch 7: Loss: 0.427, Accuracy: 0.974\n",
      "Test: Loss: 0.373, Accuracy: 0.997\n",
      "Test: Loss: 3.146, Accuracy: 0.043\n",
      "----------------------------\n",
      "Epoch 8: Loss: 0.405, Accuracy: 0.979\n",
      "Test: Loss: 0.359, Accuracy: 0.997\n",
      "Test: Loss: 2.945, Accuracy: 0.034\n",
      "----------------------------\n",
      "Epoch 9: Loss: 0.394, Accuracy: 0.986\n",
      "Test: Loss: 0.360, Accuracy: 0.994\n",
      "Test: Loss: 2.825, Accuracy: 0.053\n",
      "----------------------------\n",
      "Epoch 10: Loss: 0.381, Accuracy: 0.994\n",
      "Test: Loss: 0.362, Accuracy: 0.997\n",
      "Test: Loss: 2.990, Accuracy: 0.023\n",
      "----------------------------\n",
      "Epoch 11: Loss: 0.379, Accuracy: 0.992\n",
      "Test: Loss: 0.363, Accuracy: 0.994\n",
      "Test: Loss: 3.019, Accuracy: 0.014\n",
      "----------------------------\n",
      "PREPROCESSING LEVEL:  2 FOLD:  2\n",
      "Epoch 0: Loss: 1.207, Accuracy: 0.550\n",
      "Test: Loss: 0.802, Accuracy: 0.889\n",
      "Test: Loss: 1.677, Accuracy: 0.128\n",
      "----------------------------\n",
      "Epoch 1: Loss: 0.677, Accuracy: 0.864\n",
      "Test: Loss: 0.443, Accuracy: 0.969\n",
      "Test: Loss: 2.333, Accuracy: 0.090\n",
      "----------------------------\n",
      "Epoch 2: Loss: 0.558, Accuracy: 0.901\n",
      "Test: Loss: 0.424, Accuracy: 0.980\n",
      "Test: Loss: 2.518, Accuracy: 0.049\n",
      "----------------------------\n",
      "Epoch 3: Loss: 0.531, Accuracy: 0.914\n",
      "Test: Loss: 0.530, Accuracy: 0.917\n",
      "Test: Loss: 2.591, Accuracy: 0.100\n",
      "----------------------------\n",
      "Epoch 4: Loss: 0.485, Accuracy: 0.949\n",
      "Test: Loss: 0.393, Accuracy: 0.993\n",
      "Test: Loss: 3.070, Accuracy: 0.014\n",
      "----------------------------\n",
      "Epoch 5: Loss: 0.455, Accuracy: 0.955\n",
      "Test: Loss: 0.384, Accuracy: 0.990\n",
      "Test: Loss: 2.738, Accuracy: 0.019\n",
      "----------------------------\n",
      "Epoch 6: Loss: 0.448, Accuracy: 0.962\n",
      "Test: Loss: 0.400, Accuracy: 0.986\n",
      "Test: Loss: 3.103, Accuracy: 0.013\n",
      "----------------------------\n",
      "Epoch 7: Loss: 0.420, Accuracy: 0.975\n",
      "Test: Loss: 0.382, Accuracy: 0.987\n",
      "Test: Loss: 3.074, Accuracy: 0.025\n",
      "----------------------------\n",
      "Epoch 8: Loss: 0.416, Accuracy: 0.971\n",
      "Test: Loss: 0.382, Accuracy: 0.990\n",
      "Test: Loss: 2.837, Accuracy: 0.041\n",
      "----------------------------\n",
      "Epoch 9: Loss: 0.401, Accuracy: 0.981\n",
      "Test: Loss: 0.378, Accuracy: 0.987\n",
      "Test: Loss: 3.028, Accuracy: 0.020\n",
      "----------------------------\n",
      "Epoch 10: Loss: 0.396, Accuracy: 0.984\n",
      "Test: Loss: 0.377, Accuracy: 0.990\n",
      "Test: Loss: 3.058, Accuracy: 0.017\n",
      "----------------------------\n",
      "Epoch 11: Loss: 0.397, Accuracy: 0.985\n",
      "Test: Loss: 0.376, Accuracy: 0.990\n",
      "Test: Loss: 3.052, Accuracy: 0.020\n",
      "----------------------------\n",
      "PREPROCESSING LEVEL:  2 FOLD:  3\n",
      "Epoch 0: Loss: 1.210, Accuracy: 0.534\n",
      "Test: Loss: 0.798, Accuracy: 0.870\n",
      "Test: Loss: 1.688, Accuracy: 0.047\n",
      "----------------------------\n",
      "Epoch 1: Loss: 0.671, Accuracy: 0.861\n",
      "Test: Loss: 0.429, Accuracy: 0.987\n",
      "Test: Loss: 2.477, Accuracy: 0.039\n",
      "----------------------------\n",
      "Epoch 2: Loss: 0.544, Accuracy: 0.919\n",
      "Test: Loss: 0.395, Accuracy: 0.993\n",
      "Test: Loss: 3.269, Accuracy: 0.006\n",
      "----------------------------\n",
      "Epoch 3: Loss: 0.530, Accuracy: 0.928\n",
      "Test: Loss: 0.391, Accuracy: 0.990\n",
      "Test: Loss: 2.870, Accuracy: 0.032\n",
      "----------------------------\n",
      "Epoch 4: Loss: 0.487, Accuracy: 0.940\n",
      "Test: Loss: 0.370, Accuracy: 1.000\n",
      "Test: Loss: 3.138, Accuracy: 0.004\n",
      "----------------------------\n",
      "Epoch 5: Loss: 0.483, Accuracy: 0.944\n",
      "Test: Loss: 0.368, Accuracy: 1.000\n",
      "Test: Loss: 2.977, Accuracy: 0.030\n",
      "----------------------------\n",
      "Epoch 6: Loss: 0.436, Accuracy: 0.965\n",
      "Test: Loss: 0.367, Accuracy: 1.000\n",
      "Test: Loss: 2.995, Accuracy: 0.009\n",
      "----------------------------\n",
      "Epoch 7: Loss: 0.435, Accuracy: 0.962\n",
      "Test: Loss: 0.360, Accuracy: 1.000\n",
      "Test: Loss: 2.934, Accuracy: 0.005\n",
      "----------------------------\n",
      "Epoch 8: Loss: 0.400, Accuracy: 0.982\n",
      "Test: Loss: 0.363, Accuracy: 0.997\n",
      "Test: Loss: 2.903, Accuracy: 0.008\n",
      "----------------------------\n",
      "Epoch 9: Loss: 0.399, Accuracy: 0.985\n",
      "Test: Loss: 0.361, Accuracy: 0.997\n",
      "Test: Loss: 2.853, Accuracy: 0.013\n",
      "----------------------------\n",
      "Epoch 10: Loss: 0.398, Accuracy: 0.985\n",
      "Test: Loss: 0.360, Accuracy: 0.997\n",
      "Test: Loss: 2.830, Accuracy: 0.018\n",
      "----------------------------\n",
      "Epoch 11: Loss: 0.386, Accuracy: 0.989\n",
      "Test: Loss: 0.361, Accuracy: 0.993\n",
      "Test: Loss: 2.822, Accuracy: 0.016\n",
      "----------------------------\n",
      "PREPROCESSING LEVEL:  2 FOLD:  4\n",
      "Epoch 0: Loss: 1.218, Accuracy: 0.539\n",
      "Test: Loss: 0.773, Accuracy: 0.911\n",
      "Test: Loss: 1.604, Accuracy: 0.064\n",
      "----------------------------\n",
      "Epoch 1: Loss: 0.656, Accuracy: 0.874\n",
      "Test: Loss: 0.423, Accuracy: 0.980\n",
      "Test: Loss: 2.260, Accuracy: 0.080\n",
      "----------------------------\n",
      "Epoch 2: Loss: 0.538, Accuracy: 0.919\n",
      "Test: Loss: 0.451, Accuracy: 0.957\n",
      "Test: Loss: 2.855, Accuracy: 0.014\n",
      "----------------------------\n",
      "Epoch 3: Loss: 0.510, Accuracy: 0.929\n",
      "Test: Loss: 0.472, Accuracy: 0.941\n",
      "Test: Loss: 2.700, Accuracy: 0.070\n",
      "----------------------------\n",
      "Epoch 4: Loss: 0.484, Accuracy: 0.935\n",
      "Test: Loss: 0.381, Accuracy: 1.000\n",
      "Test: Loss: 2.880, Accuracy: 0.082\n",
      "----------------------------\n",
      "Epoch 5: Loss: 0.478, Accuracy: 0.945\n",
      "Test: Loss: 0.450, Accuracy: 0.954\n",
      "Test: Loss: 3.015, Accuracy: 0.009\n",
      "----------------------------\n",
      "Epoch 6: Loss: 0.449, Accuracy: 0.960\n",
      "Test: Loss: 0.365, Accuracy: 1.000\n",
      "Test: Loss: 2.867, Accuracy: 0.010\n",
      "----------------------------\n",
      "Epoch 7: Loss: 0.413, Accuracy: 0.978\n",
      "Test: Loss: 0.377, Accuracy: 0.990\n",
      "Test: Loss: 2.901, Accuracy: 0.010\n",
      "----------------------------\n",
      "Epoch 8: Loss: 0.407, Accuracy: 0.979\n",
      "Test: Loss: 0.359, Accuracy: 1.000\n",
      "Test: Loss: 2.923, Accuracy: 0.068\n",
      "----------------------------\n",
      "Epoch 9: Loss: 0.395, Accuracy: 0.986\n",
      "Test: Loss: 0.355, Accuracy: 1.000\n",
      "Test: Loss: 2.776, Accuracy: 0.109\n",
      "----------------------------\n",
      "Epoch 10: Loss: 0.392, Accuracy: 0.984\n",
      "Test: Loss: 0.355, Accuracy: 1.000\n",
      "Test: Loss: 2.925, Accuracy: 0.070\n",
      "----------------------------\n",
      "Epoch 11: Loss: 0.385, Accuracy: 0.989\n",
      "Test: Loss: 0.355, Accuracy: 1.000\n",
      "Test: Loss: 2.965, Accuracy: 0.045\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "path = \"/content/drive/MyDrive/PhD/WingApplication_V3/\"\n",
    "for preprocessing_level in range(4):\n",
    "    for FOLD in range(5):\n",
    "        print(\"PREPROCESSING LEVEL: \", preprocessing_level, \"FOLD: \", FOLD)\n",
    "        train_dataloader, test_dataloader, ood_dataloader, oh_encoder, train_path_list, test_path_list, ood_test_path_list = prepare_data(FOLD, preprocessing_level)\n",
    "        model = load_model(preprocessing_level)\n",
    "        # Define loss\n",
    "        ce_loss = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        # Define optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "        # TRAIN\n",
    "        for t in range(epochs):\n",
    "            train_loop(train_dataloader, model, t, ce_loss, optimizer, scheduler)\n",
    "            test_loop(test_dataloader, model, ce_loss)\n",
    "            test_loop(ood_dataloader, model, ce_loss)\n",
    "            print(\"----------------------------\")\n",
    "\n",
    "        # TEST & SAVE\n",
    "        test_df = pd.DataFrame()\n",
    "\n",
    "        predictions, targets = evaluation(test_dataloader, model)\n",
    "        predictions_ood, targets_ood = evaluation(ood_dataloader, model)\n",
    "\n",
    "        test_df[\"PATH\"] = list(test_path_list) + list(ood_test_path_list)\n",
    "        test_df[\"PRED\"] = list(oh_encoder.inverse_transform(predictions)) + list(oh_encoder.inverse_transform(predictions_ood))\n",
    "        test_df[\"TARGET\"] = list(oh_encoder.inverse_transform(targets)) + list(oh_encoder.inverse_transform(targets_ood))\n",
    "\n",
    "        path = \"/content/drive/MyDrive/PhD/WingApplication_V3/\"\n",
    "        pd.to_pickle(test_df, path + \"results/{}/test_df_{}_{}-{}.pkl\".format(EXPERIMENT, EXPERIMENT, FOLD, preprocessing_level))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
